{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13388097,"sourceType":"datasetVersion","datasetId":8495034}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:39:59.769543Z","iopub.execute_input":"2025-10-15T08:39:59.769714Z","iopub.status.idle":"2025-10-15T08:40:00.057395Z","shell.execute_reply.started":"2025-10-15T08:39:59.769696Z","shell.execute_reply":"2025-10-15T08:40:00.056616Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/polypharmacy-dataset/Side_effects_unique.csv\n/kaggle/input/polypharmacy-dataset/neg.csv\n/kaggle/input/polypharmacy-dataset/Drugbank_ID_SMILE_all_structure links.csv\n/kaggle/input/polypharmacy-dataset/pos.csv\n/kaggle/input/polypharmacy-dataset/DrugBankID2SMILES.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install rdkit torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:00.058831Z","iopub.execute_input":"2025-10-15T08:40:00.059114Z","iopub.status.idle":"2025-10-15T08:40:07.018675Z","shell.execute_reply.started":"2025-10-15T08:40:00.059097Z","shell.execute_reply":"2025-10-15T08:40:07.017850Z"}},"outputs":[{"name":"stdout","text":"Collecting rdkit\n  Downloading rdkit-2025.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nCollecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.3.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.15)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.9.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.1.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.8.3)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit) (2024.2.0)\nDownloading rdkit-2025.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (36.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric, rdkit\nSuccessfully installed rdkit-2025.9.1 torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# %% Cell 1 — Imports & device\nimport os, ast, math, time, json\nfrom pathlib import Path\nfrom itertools import combinations\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# RDKit for fingerprints\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\n\n# sklearn utils\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, normalize\nfrom sklearn.model_selection import GroupShuffleSplit, train_test_split\nfrom sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, f1_score, confusion_matrix\n\n# torch + pyg\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\n# try imports for torch_geometric\ntry:\n    from torch_geometric.data import Data\n    from torch_geometric.nn import SAGEConv, global_mean_pool\nexcept Exception as e:\n    raise ImportError(\"torch_geometric not available. Install it before running. Error: \" + str(e))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:07.019834Z","iopub.execute_input":"2025-10-15T08:40:07.020105Z","iopub.status.idle":"2025-10-15T08:40:17.878709Z","shell.execute_reply.started":"2025-10-15T08:40:07.020081Z","shell.execute_reply":"2025-10-15T08:40:17.878045Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# %% Cell 2 — Paths to files (Kaggle dataset)\nDATA_DIR = Path('/kaggle/input/polypharmacy-dataset')\nSIDE_EFFECTS_CSV = DATA_DIR / 'Side_effects_unique.csv'\nDRUG_SMILES_CSV = DATA_DIR / 'DrugBankID2SMILES.csv'\nPOS_CSV = DATA_DIR / 'pos.csv'\nNEG_CSV = DATA_DIR / 'neg.csv'\n\nfor p in [SIDE_EFFECTS_CSV, DRUG_SMILES_CSV, POS_CSV, NEG_CSV]:\n    print(p, \"exists?\", p.exists())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:17.879443Z","iopub.execute_input":"2025-10-15T08:40:17.879870Z","iopub.status.idle":"2025-10-15T08:40:17.886068Z","shell.execute_reply.started":"2025-10-15T08:40:17.879849Z","shell.execute_reply":"2025-10-15T08:40:17.885378Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/polypharmacy-dataset/Side_effects_unique.csv exists? True\n/kaggle/input/polypharmacy-dataset/DrugBankID2SMILES.csv exists? True\n/kaggle/input/polypharmacy-dataset/pos.csv exists? True\n/kaggle/input/polypharmacy-dataset/neg.csv exists? True\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# %% Cell 3 — Load side-effect embeddings and normalize\nse_df = pd.read_csv(SIDE_EFFECTS_CSV, low_memory=False)\n# first two columns are UMLS and name; rest numeric\nse_ids = se_df.iloc[:, 0].astype(str).tolist()\nse_names = se_df.iloc[:, 1].astype(str).tolist()\nse_vectors = se_df.iloc[:, 2:].apply(pd.to_numeric, errors='coerce').fillna(0).values\nprint(\"SE matrix shape:\", se_vectors.shape)\n\n# Standardize (z-score)\nse_scaler = StandardScaler()\nse_vectors_scaled = se_scaler.fit_transform(se_vectors)\n# store mapping\nSE_MAP = {uid: vec for uid, vec in zip(se_ids, se_vectors_scaled)}\nSE_NAME_MAP = {uid: name for uid, name in zip(se_ids, se_names)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:17.886858Z","iopub.execute_input":"2025-10-15T08:40:17.887135Z","iopub.status.idle":"2025-10-15T08:40:20.347961Z","shell.execute_reply.started":"2025-10-15T08:40:17.887113Z","shell.execute_reply":"2025-10-15T08:40:20.347226Z"}},"outputs":[{"name":"stdout","text":"SE matrix shape: (7350, 768)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 🧹 Add this at the very top of your notebook to silence RDKit warnings\nfrom rdkit import RDLogger\nRDLogger.DisableLog('rdApp.*')       # hides all RDKit info/warning/deprecation logs\nimport warnings\nwarnings.filterwarnings(\"ignore\")    # hides Python-level warnings too\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:20.348647Z","iopub.execute_input":"2025-10-15T08:40:20.348854Z","iopub.status.idle":"2025-10-15T08:40:20.353180Z","shell.execute_reply.started":"2025-10-15T08:40:20.348836Z","shell.execute_reply":"2025-10-15T08:40:20.352436Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# %% Cell 4 replacement — fingerprints + RDKit molecular descriptors + cache\nfrom rdkit.Chem import Descriptors\nfrom rdkit import Chem, DataStructs, RDLogger\nRDLogger.DisableLog('rdApp.*')\n\nDRUG_SMILES_CSV = Path('/kaggle/input/polypharmacy-dataset/DrugBankID2SMILES.csv')\ndrug_smiles_df = pd.read_csv(DRUG_SMILES_CSV)\ndrug_smiles_df['drugbank_id'] = drug_smiles_df['drugbank_id'].astype(str)\ndrug_smiles_df['smiles'] = drug_smiles_df['smiles'].fillna('').astype(str)\n\nN_BITS = 1024\n\ndef compute_descriptors(smiles):\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return np.zeros(6, dtype=np.float32)\n        return np.array([\n            Descriptors.MolWt(mol),\n            Descriptors.TPSA(mol),\n            Descriptors.MolLogP(mol),\n            Descriptors.NumHDonors(mol),\n            Descriptors.NumHAcceptors(mol),\n            Descriptors.NumRotatableBonds(mol)\n        ], dtype=np.float32)\n    except Exception:\n        return np.zeros(6, dtype=np.float32)\n\ndef smiles_to_ecfp_bits_one(smiles, nBits=N_BITS):\n    if not isinstance(smiles, str) or smiles.strip()==\"\":\n        return np.zeros(nBits, dtype=np.uint8), True\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return np.zeros(nBits, dtype=np.uint8), True\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=nBits)\n        arr = np.zeros((nBits,), dtype=np.uint8)\n        DataStructs.ConvertToNumpyArray(fp, arr)\n        return arr, False\n    except Exception:\n        return np.zeros(nBits, dtype=np.uint8), True\n\n# compute/cached\nCACHE_FP = Path('/kaggle/working/drug_fp_desc.npz')\nif CACHE_FP.exists():\n    cache = np.load(CACHE_FP, allow_pickle=True)\n    drug_ids = cache['drug_ids'].tolist()\n    fps = cache['fps']\n    descs = cache['descs']\n    miss_flags = cache['miss_flags'].tolist()\nelse:\n    drug_ids = drug_smiles_df['drugbank_id'].tolist()\n    smiles_list = drug_smiles_df['smiles'].tolist()\n    from joblib import Parallel, delayed\n    results = Parallel(n_jobs=min(12, (os.cpu_count() or 1)), backend='loky')(\n        delayed(lambda s: (smiles_to_ecfp_bits_one(s), compute_descriptors(s)))(s) for s in smiles_list\n    )\n    fps = np.stack([r[0][0] for r in results], axis=0)\n    miss_flags = [r[0][1] for r in results]\n    descs = np.stack([r[1] for r in results], axis=0)\n    np.savez_compressed(CACHE_FP, drug_ids=drug_ids, fps=fps, descs=descs, miss_flags=miss_flags)\n\ndrug_fp_map = {db: fps[i] for i, db in enumerate(drug_ids)}\ndrug_desc_map = {db: descs[i] for i, db in enumerate(drug_ids)}\nmissing_smiles_flag = {db: bool(miss_flags[i]) for i, db in enumerate(drug_ids)}\n\nprint(\"Loaded/cached fingerprints + descriptors for\", len(drug_ids))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 5 — Load pos/neg and expand hyperedges to pairwise edges (clique expansion)\npos_df = pd.read_csv(POS_CSV)\nneg_df = pd.read_csv(NEG_CSV)\n\ndef parse_drug_list(s):\n    # safe parse of \"['DB0001','DB0002']\"\n    try:\n        return [x.strip().strip(\"'\\\" \") for x in ast.literal_eval(s)]\n    except Exception:\n        # fallback heuristics\n        s2 = s.strip().strip('[]')\n        return [x.strip().strip(\"'\\\" \") for x in s2.split(',') if x.strip()]\n\ndef hyperedges_to_pairs(df, label):\n    rows = []\n    for _, r in df.iterrows():\n        drugs = parse_drug_list(r['DrugBankID'])\n        time_col = r.get('time', None)\n        report_id = r.get('report_id', None)\n        se_cui = r.get('SE_above_0.9', None)\n        # create all unordered pairs (i<j)\n        for a,b in combinations(sorted(set(drugs)), 2):\n            rows.append({'drug_a': a, 'drug_b': b, 'label': label, 'time': time_col, 'report_id': report_id, 'se_cui': se_cui})\n    return pd.DataFrame(rows)\n\npos_pairs = hyperedges_to_pairs(pos_df, 1)\nneg_pairs = hyperedges_to_pairs(neg_df, 0)\nedges_df = pd.concat([pos_pairs, neg_pairs], ignore_index=True).drop_duplicates(subset=['drug_a','drug_b','report_id','time'])\nprint(\"Edges shape:\", edges_df.shape)\nprint(edges_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:29.081990Z","iopub.execute_input":"2025-10-15T08:40:29.082268Z","iopub.status.idle":"2025-10-15T08:40:58.393201Z","shell.execute_reply.started":"2025-10-15T08:40:29.082239Z","shell.execute_reply":"2025-10-15T08:40:58.392375Z"}},"outputs":[{"name":"stdout","text":"Edges shape: (2548128, 6)\n    drug_a   drug_b  label    time report_id    se_cui\n0  DB00273  DB00472      1  2015Q4  11809573  C0151878\n1  DB00273  DB00555      1  2015Q4  11809573  C0151878\n2  DB00273  DB00557      1  2015Q4  11809573  C0151878\n3  DB00273  DB00564      1  2015Q4  11809573  C0151878\n4  DB00273  DB01050      1  2015Q4  11809573  C0151878\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# %% Cell 6 replacement — build node features and compress\nunique_drugs = sorted(set(edges_df['drug_a']).union(set(edges_df['drug_b'])))\nprint(\"Unique drugs:\", len(unique_drugs))\n\n# build fingerprint + descriptor matrices aligned to unique_drugs\nfps_mat = np.stack([drug_fp_map.get(d, np.zeros(N_BITS, dtype=np.uint8)).astype(np.float32) for d in unique_drugs])\ndesc_mat = np.stack([drug_desc_map.get(d, np.zeros(6, dtype=np.float32)) for d in unique_drugs])\n\n# robust scaling for fps then SVD compress + concat desc (normed)\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nrobust = RobustScaler()\nfps_scaled = robust.fit_transform(fps_mat)  # (N, N_BITS)\n\n# SVD on fingerprint to reduce to 256 dims\nSVD_DIM = 256\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=SVD_DIM, random_state=42)\nfps_svd = svd.fit_transform(fps_scaled)  # (N, 256)\n\n# standardize descriptors and concat\ndesc_scaler = StandardScaler()\ndesc_scaled = desc_scaler.fit_transform(desc_mat)  # (N,6)\nX_node = np.hstack([fps_svd, desc_scaled])  # final node features (N, 256+6 = 262)\nprint(\"Node features shape:\", X_node.shape)\n\n# overwrite X_fp_svd used downstream\nX_fp_svd = X_node.astype(np.float32)\ndrug_to_idx = {d: i for i, d in enumerate(unique_drugs)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:40:58.394082Z","iopub.execute_input":"2025-10-15T08:40:58.394378Z","iopub.status.idle":"2025-10-15T08:41:01.375388Z","shell.execute_reply.started":"2025-10-15T08:40:58.394349Z","shell.execute_reply":"2025-10-15T08:41:01.374695Z"}},"outputs":[{"name":"stdout","text":"Unique drugs: 12298\nNode features shape: (12298, 262)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# %% Cell 7 replacement — scale node features after SVD (important)\nfrom sklearn.preprocessing import StandardScaler\n\n# X_fp_svd already computed\nscaler_node = StandardScaler()\nX_fp_svd = scaler_node.fit_transform(X_fp_svd)   # overwrite with scaled version\nprint(\"Node feature shape after SVD & scaling:\", X_fp_svd.shape)\n\ndrug_to_idx = {d:i for i,d in enumerate(unique_drugs)}\n\n# Save scaler for later use\nimport joblib\njoblib.dump(scaler_node, \"/kaggle/working/node_scaler.joblib\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:41:01.376557Z","iopub.execute_input":"2025-10-15T08:41:01.377200Z","iopub.status.idle":"2025-10-15T08:41:01.449681Z","shell.execute_reply.started":"2025-10-15T08:41:01.377164Z","shell.execute_reply":"2025-10-15T08:41:01.449090Z"}},"outputs":[{"name":"stdout","text":"Node feature shape after SVD & scaling: (12298, 262)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/node_scaler.joblib']"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# %% Cell 8 replacement — rebuild edges and do time-aware / group split (robust)\nimport ast\nfrom itertools import combinations\nfrom sklearn.model_selection import GroupShuffleSplit, train_test_split\nimport numpy as np\n\n# --- helper to safely parse drug lists ---\ndef parse_drug_list(s):\n    try:\n        return [x.strip().strip(\"'\\\" \") for x in ast.literal_eval(s)]\n    except Exception:\n        s2 = str(s).strip().strip('[]')\n        if s2 == '' or s2.lower() == 'nan':\n            return []\n        return [x.strip().strip(\"'\\\" \") for x in s2.split(',') if x.strip()]\n\n# --- (re)create edges_df if needed ---\nif 'edges_df' not in globals():\n    # read pos/neg if not loaded yet\n    if 'pos_df' not in globals():\n        pos_df = pd.read_csv(POS_CSV, low_memory=False)\n    if 'neg_df' not in globals():\n        neg_df = pd.read_csv(NEG_CSV, low_memory=False)\n\n    def hyperedges_to_pairs(df, label):\n        rows = []\n        for _, r in df.iterrows():\n            drugs = parse_drug_list(r.get('DrugBankID', r.get('drug_list', '[]')))\n            if len(drugs) < 2:\n                continue\n            time_col = r.get('time', None)\n            report_id = r.get('report_id', None)\n            se_cui = r.get('SE_above_0.9', r.get('se_cui', None))\n            for a,b in combinations(sorted(set(drugs)), 2):\n                rows.append({'drug_a': a, 'drug_b': b, 'label': int(label),\n                             'time': time_col, 'report_id': report_id, 'se_cui': se_cui})\n        return pd.DataFrame(rows)\n\n    pos_pairs = hyperedges_to_pairs(pos_df, 1)\n    neg_pairs = hyperedges_to_pairs(neg_df, 0)\n    edges_df = pd.concat([pos_pairs, neg_pairs], ignore_index=True).drop_duplicates(subset=['drug_a','drug_b','report_id','time'])\n    print(\"Rebuilt edges_df shape:\", edges_df.shape)\nelse:\n    print(\"Using existing edges_df with shape:\", edges_df.shape)\n\n# --- create edge_labels from edges_df if missing ---\nedge_labels = edges_df[['drug_a','drug_b','label','time','report_id','se_cui']].copy()\n\n# --- ensure drug_to_idx and node indexing exist; if not create from present drugs ---\nif 'drug_to_idx' not in globals():\n    unique_drugs = sorted(set(edge_labels['drug_a']).union(set(edge_labels['drug_b'])))\n    drug_to_idx = {d:i for i,d in enumerate(unique_drugs)}\n    print(\"Created drug_to_idx for\", len(unique_drugs), \"drugs\")\n\n# map to integer node ids 'u' and 'v'\nedge_labels['u'] = edge_labels['drug_a'].map(drug_to_idx).astype('Int64')\nedge_labels['v'] = edge_labels['drug_b'].map(drug_to_idx).astype('Int64')\n\n# drop any rows with missing mapping (if some drug not found)\nbefore = len(edge_labels)\nedge_labels = edge_labels.dropna(subset=['u','v']).reset_index(drop=True)\nafter = len(edge_labels)\nif after < before:\n    print(f\"Dropped {before-after} edges due to missing drug->idx mapping\")\n\n# --- Now do time-aware split if time exists, else group split by report_id ---\ndef time_to_ordinal(t):\n    try:\n        ts = str(t)\n        if 'Q' in ts:\n            y,q = ts.split('Q')\n            return int(y)*10 + int(q)\n        return int(float(ts))\n    except Exception:\n        return 0\n\nif 'time' in edge_labels.columns and edge_labels['time'].notnull().any():\n    edge_labels['time_ord'] = edge_labels['time'].apply(time_to_ordinal)\n    times_sorted = sorted(edge_labels['time_ord'].unique())\n    cutoff_idx = max(1, int(len(times_sorted)*0.7))\n    cutoff_time = times_sorted[min(cutoff_idx, len(times_sorted)-1)]\n    train_mask = edge_labels['time_ord'] <= cutoff_time\n    rest = ~train_mask\n    rest_idx = edge_labels[rest].index\n    # stratify only if both classes present in 'rest'\n    stratify_param = edge_labels.loc[rest, 'label'] if edge_labels.loc[rest, 'label'].nunique() > 1 else None\n    val_idx, test_idx = train_test_split(rest_idx, test_size=0.5, random_state=42, stratify=stratify_param)\n    val_mask = edge_labels.index.isin(val_idx)\n    test_mask = edge_labels.index.isin(test_idx)\nelse:\n    gss = GroupShuffleSplit(n_splits=1, train_size=0.7, random_state=42)\n    groups = edge_labels['report_id'].fillna('_no_report_')\n    train_idx, other_idx = next(gss.split(edge_labels, edge_labels['label'], groups=groups))\n    # stratify only if both classes present in other_idx\n    strat = edge_labels.loc[other_idx, 'label'] if edge_labels.loc[other_idx, 'label'].nunique() > 1 else None\n    val_idx, test_idx = train_test_split(other_idx, test_size=0.5, random_state=42, stratify=strat)\n    train_mask = edge_labels.index.isin(train_idx)\n    val_mask = edge_labels.index.isin(val_idx)\n    test_mask = edge_labels.index.isin(test_idx)\n\nedge_labels['split'] = np.where(train_mask, 'train', np.where(val_mask, 'val', 'test'))\nprint(edge_labels['split'].value_counts())\n# expose train/val/test dfs\ntrain_df = edge_labels[edge_labels['split']=='train'].reset_index(drop=True)\nval_df = edge_labels[edge_labels['split']=='val'].reset_index(drop=True)\ntest_df = edge_labels[edge_labels['split']=='test'].reset_index(drop=True)\nprint(\"train/val/test sizes:\", len(train_df), len(val_df), len(test_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:41:01.450438Z","iopub.execute_input":"2025-10-15T08:41:01.451018Z","iopub.status.idle":"2025-10-15T08:41:07.385454Z","shell.execute_reply.started":"2025-10-15T08:41:01.450991Z","shell.execute_reply":"2025-10-15T08:41:07.384630Z"}},"outputs":[{"name":"stdout","text":"Using existing edges_df with shape: (2548128, 6)\nsplit\ntrain    1954270\nval       296929\ntest      296929\nName: count, dtype: int64\ntrain/val/test sizes: 1954270 296929 296929\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# %% REPLACE Cell 9 — build edge_index_t from ONLY TRAINING POSITIVE EDGES\nimport torch\nfrom torch_geometric.utils import remove_self_loops\n\n# Build graph using ONLY POSITIVE TRAINING edges (no data leakage)\ntrain_pos_edges = train_df[train_df['label']==1][['u','v']].values\n\n# Stack both directions for undirected graph\nedge_index_np = np.vstack([\n    np.concatenate([train_pos_edges[:,0], train_pos_edges[:,1]]), \n    np.concatenate([train_pos_edges[:,1], train_pos_edges[:,0]])\n]).astype(np.int64)\n\n# Remove self-loops\nedge_index_tensor = torch.tensor(edge_index_np, dtype=torch.long)\nedge_index_tensor, _ = remove_self_loops(edge_index_tensor)\n\n# Create Data object with correct graph\ndata = Data(x=torch.tensor(X_fp_svd, dtype=torch.float), edge_index=edge_index_tensor).to(device)\nedge_index_t = edge_index_tensor\n\nprint(\"✅ CORRECTED: Graph built with only positive training edges\")\nprint(\"Data created:\", data)\nprint(\"Num nodes:\", data.num_nodes, \"Num edges (directed, positive only):\", data.num_edges)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:59:36.835780Z","iopub.execute_input":"2025-10-15T08:59:36.836411Z","iopub.status.idle":"2025-10-15T08:59:37.046624Z","shell.execute_reply.started":"2025-10-15T08:59:36.836387Z","shell.execute_reply":"2025-10-15T08:59:37.045833Z"}},"outputs":[{"name":"stdout","text":"✅ CORRECTED: Graph built with only positive training edges\nData created: Data(x=[12298, 262], edge_index=[2, 1574422])\nNum nodes: 12298 Num edges (directed, positive only): 1574422\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# %% REPLACE Cell 10 — Improved faster encoder + classifier\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\nclass ImprovedGraphSAGE(nn.Module):\n    def __init__(self, in_channels, hidden_channels=256, n_layers=2, dropout=0.3):\n        super().__init__()\n        self.convs = nn.ModuleList()\n        self.convs.append(SAGEConv(in_channels, hidden_channels))\n        for _ in range(n_layers-1):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n        self.dropout = nn.Dropout(dropout)\n        self.act = nn.ReLU()\n    \n    def forward(self, x, edge_index):\n        for conv in self.convs:\n            x = conv(x, edge_index)\n            x = self.act(x)\n            x = self.dropout(x)\n        return x\n\nclass ImprovedEdgeClassifier(nn.Module):\n    def __init__(self, node_emb_dim, hidden=128, dropout=0.3):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(node_emb_dim * 2, hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, 1)\n        )\n    \n    def forward(self, ha, hb):\n        h = torch.cat([ha, hb], dim=1)\n        return self.mlp(h).squeeze(1)\n\n# Initialize improved models\nin_dim = X_fp_svd.shape[1]\nencoder = ImprovedGraphSAGE(in_dim, hidden_channels=256, n_layers=2).to(device)\nedge_clf = ImprovedEdgeClassifier(256).to(device)\n\nprint(\"✅ CORRECTED: Using improved faster architecture\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:59:52.896435Z","iopub.execute_input":"2025-10-15T08:59:52.897116Z","iopub.status.idle":"2025-10-15T08:59:52.914319Z","shell.execute_reply.started":"2025-10-15T08:59:52.897091Z","shell.execute_reply":"2025-10-15T08:59:52.913613Z"}},"outputs":[{"name":"stdout","text":"✅ CORRECTED: Using improved faster architecture\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# %% Cell 11 replacement — balanced batches with oversampling (more positive presence)\ntrain_df = edge_labels[edge_labels['split']=='train'].reset_index(drop=True)\nval_df = edge_labels[edge_labels['split']=='val'].reset_index(drop=True)\ntest_df = edge_labels[edge_labels['split']=='test'].reset_index(drop=True)\n\ntrain_df['u'] = train_df['u'].astype(int); train_df['v'] = train_df['v'].astype(int)\nval_df['u'] = val_df['u'].astype(int); val_df['v'] = val_df['v'].astype(int)\ntest_df['u'] = test_df['u'].astype(int); test_df['v'] = test_df['v'].astype(int)\n\ntrain_pos_idx = train_df[train_df['label']==1].index.to_numpy()\ntrain_neg_idx = train_df[train_df['label']==0].index.to_numpy()\n\n# oversample positives to reach a target positive ratio per batch (e.g., 40%)\ndef balanced_edge_batches(df, pos_idx, neg_idx, batch_size=4096, pos_ratio=0.4):\n    pos_per = int(batch_size * pos_ratio)\n    neg_per = batch_size - pos_per\n    # create repeated pools\n    while True:\n        p = np.random.choice(pos_idx, size=max(len(pos_idx), pos_per*2), replace=True)\n        n = np.random.choice(neg_idx, size=max(len(neg_idx), neg_per*2), replace=True)\n        # iterate in chunks\n        for i in range(0, max(len(p), len(n)), max(pos_per, neg_per)):\n            pos_chunk = p[i:i+pos_per]\n            neg_chunk = n[i:i+neg_per]\n            chosen = np.concatenate([pos_chunk, neg_chunk])\n            np.random.shuffle(chosen)\n            sub = df.loc[chosen]\n            u = torch.tensor(sub['u'].values, dtype=torch.long, device=device)\n            v = torch.tensor(sub['v'].values, dtype=torch.long, device=device)\n            y = torch.tensor(sub['label'].astype(np.float32).values, dtype=torch.float, device=device)\n            yield u, v, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:41:39.834911Z","iopub.execute_input":"2025-10-15T08:41:39.835521Z","iopub.status.idle":"2025-10-15T08:41:41.933068Z","shell.execute_reply.started":"2025-10-15T08:41:39.835494Z","shell.execute_reply":"2025-10-15T08:41:41.932285Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# %% REPLACE Cell 12 — Optimized training (30 min target)\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nimport time\nimport numpy as np\n\n# Configuration\nEPOCHS = 25  # Reduced for speed\nBATCH_SIZE = 8192\nLR = 0.001\n\n# Pre-compute indices for faster sampling\ntrain_pos_idx = train_df[train_df['label']==1].index.values\ntrain_neg_idx = train_df[train_df['label']==0].index.values\n\n# Optimizer\noptimizer = torch.optim.Adam(\n    list(encoder.parameters()) + list(edge_clf.parameters()), \n    lr=LR, weight_decay=1e-5\n)\n\n# Class weighting for imbalance\npos_weight = torch.tensor([len(train_neg_idx) / len(train_pos_idx)], device=device)\n\n# Mixed precision for speed\nscaler = torch.cuda.amp.GradScaler()\n\nbest_val_auc = 0\npatience_counter = 0\npatience = 5\n\nprint(\"🚀 Starting optimized training (target: 30 minutes)...\")\nstart_time = time.time()\n\nfor epoch in range(1, EPOCHS + 1):\n    encoder.train()\n    edge_clf.train()\n    \n    epoch_loss = 0\n    num_batches = 0\n    \n    # Fast edge sampling (no subgraph building)\n    for _ in range(0, len(train_df) // BATCH_SIZE):\n        # Sample balanced batch\n        pos_batch = train_df.loc[np.random.choice(train_pos_idx, BATCH_SIZE//2, replace=True)]\n        neg_batch = train_df.loc[np.random.choice(train_neg_idx, BATCH_SIZE//2, replace=True)]\n        batch = pd.concat([pos_batch, neg_batch]).sample(frac=1).reset_index(drop=True)\n        \n        u = torch.tensor(batch['u'].values, device=device)\n        v = torch.tensor(batch['v'].values, device=device)\n        y = torch.tensor(batch['label'].values, dtype=torch.float, device=device)\n        \n        optimizer.zero_grad()\n        \n        # Mixed precision forward\n        with torch.cuda.amp.autocast():\n            # Single forward pass on full graph (FAST)\n            node_emb = encoder(data.x, data.edge_index)\n            logits = edge_clf(node_emb[u], node_emb[v])\n            loss = F.binary_cross_entropy_with_logits(logits, y, pos_weight=pos_weight)\n        \n        # Scaled backward\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        epoch_loss += loss.item()\n        num_batches += 1\n    \n    # Validation (every epoch for monitoring)\n    encoder.eval()\n    edge_clf.eval()\n    with torch.no_grad():\n        # Use subset for faster validation\n        val_sample = val_df.sample(min(50000, len(val_df)))\n        u_val = torch.tensor(val_sample['u'].values, device=device)\n        v_val = torch.tensor(val_sample['v'].values, device=device)\n        y_val = torch.tensor(val_sample['label'].values, device=device)\n        \n        node_emb = encoder(data.x, data.edge_index)\n        logits = edge_clf(node_emb[u_val], node_emb[v_val])\n        probs = torch.sigmoid(logits).cpu().numpy()\n        \n        val_auc = roc_auc_score(y_val.cpu().numpy(), probs)\n        val_ap = average_precision_score(y_val.cpu().numpy(), probs)\n        \n        elapsed = (time.time() - start_time) / 60\n        print(f\"Epoch {epoch:02d} | Loss: {epoch_loss/num_batches:.4f} | Val AUC: {val_auc:.4f} | AP: {val_ap:.4f} | Time: {elapsed:.1f}m\")\n        \n        # Early stopping\n        if val_auc > best_val_auc + 0.001:  # Small improvement threshold\n            best_val_auc = val_auc\n            patience_counter = 0\n            torch.save({\n                'encoder': encoder.state_dict(),\n                'edge_clf': edge_clf.state_dict()\n            }, \"/kaggle/working/gnn_optimized.pth\")\n            print(f\"💾 Saved new best model (AUC: {val_auc:.4f})\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"🛑 Early stopping at epoch {epoch}\")\n                break\n\n# Load best model\ncheckpoint = torch.load(\"/kaggle/working/gnn_optimized.pth\", map_location=device)\nencoder.load_state_dict(checkpoint['encoder'])\nedge_clf.load_state_dict(checkpoint['edge_clf'])\n\ntotal_time = (time.time() - start_time) / 60\nprint(f\"✅ Training completed in {total_time:.1f} minutes\")\nprint(f\"🏆 Best Validation AUC: {best_val_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:00:06.065448Z","iopub.execute_input":"2025-10-15T09:00:06.065974Z","iopub.status.idle":"2025-10-15T09:06:28.215276Z","shell.execute_reply.started":"2025-10-15T09:00:06.065952Z","shell.execute_reply":"2025-10-15T09:06:28.214492Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting optimized training (target: 30 minutes)...\nEpoch 01 | Loss: 0.6774 | Val AUC: 0.6455 | AP: 0.5071 | Time: 0.4m\n💾 Saved new best model (AUC: 0.6455)\nEpoch 02 | Loss: 0.6647 | Val AUC: 0.6506 | AP: 0.5115 | Time: 0.9m\n💾 Saved new best model (AUC: 0.6506)\nEpoch 03 | Loss: 0.6638 | Val AUC: 0.6500 | AP: 0.5113 | Time: 1.3m\nEpoch 04 | Loss: 0.6630 | Val AUC: 0.6487 | AP: 0.5098 | Time: 1.7m\nEpoch 05 | Loss: 0.6623 | Val AUC: 0.6515 | AP: 0.5095 | Time: 2.1m\nEpoch 06 | Loss: 0.6617 | Val AUC: 0.6516 | AP: 0.5117 | Time: 2.6m\n💾 Saved new best model (AUC: 0.6516)\nEpoch 07 | Loss: 0.6616 | Val AUC: 0.6529 | AP: 0.5116 | Time: 3.0m\n💾 Saved new best model (AUC: 0.6529)\nEpoch 08 | Loss: 0.6617 | Val AUC: 0.6525 | AP: 0.5193 | Time: 3.4m\nEpoch 09 | Loss: 0.6612 | Val AUC: 0.6502 | AP: 0.5103 | Time: 3.8m\nEpoch 10 | Loss: 0.6611 | Val AUC: 0.6561 | AP: 0.5157 | Time: 4.3m\n💾 Saved new best model (AUC: 0.6561)\nEpoch 11 | Loss: 0.6616 | Val AUC: 0.6566 | AP: 0.5143 | Time: 4.7m\nEpoch 12 | Loss: 0.6610 | Val AUC: 0.6527 | AP: 0.5166 | Time: 5.1m\nEpoch 13 | Loss: 0.6605 | Val AUC: 0.6532 | AP: 0.5105 | Time: 5.5m\nEpoch 14 | Loss: 0.6608 | Val AUC: 0.6517 | AP: 0.5143 | Time: 5.9m\nEpoch 15 | Loss: 0.6608 | Val AUC: 0.6548 | AP: 0.5157 | Time: 6.4m\n🛑 Early stopping at epoch 15\n✅ Training completed in 6.4 minutes\n🏆 Best Validation AUC: 0.6561\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!pip install torch-sparse -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n!pip install torch-geometric\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# add right before for-epoch loop\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\ntorch.set_num_threads(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T08:41:07.910834Z","iopub.status.idle":"2025-10-15T08:41:07.911137Z","shell.execute_reply.started":"2025-10-15T08:41:07.910985Z","shell.execute_reply":"2025-10-15T08:41:07.910999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Load model and predict - SIMPLE VERSION\nimport torch\nfrom torch_geometric.nn import SAGEConv\nimport numpy as np\n\n# Define model architecture (same as training)\nclass ImprovedGraphSAGE(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels=256, n_layers=2, dropout=0.3):\n        super().__init__()\n        self.convs = torch.nn.ModuleList()\n        self.convs.append(SAGEConv(in_channels, hidden_channels))\n        for _ in range(n_layers-1):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n        self.dropout = torch.nn.Dropout(dropout)\n        self.act = torch.nn.ReLU()\n    \n    def forward(self, x, edge_index):\n        for conv in self.convs:\n            x = conv(x, edge_index)\n            x = self.act(x)\n            x = self.dropout(x)\n        return x\n\nclass ImprovedEdgeClassifier(torch.nn.Module):\n    def __init__(self, node_emb_dim, hidden=128, dropout=0.3):\n        super().__init__()\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(node_emb_dim * 2, hidden),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout),\n            torch.nn.Linear(hidden, 1)\n        )\n    \n    def forward(self, ha, hb):\n        h = torch.cat([ha, hb], dim=1)\n        return self.mlp(h).squeeze(1)\n\n# Load model\nin_dim = X_fp_svd.shape[1]\nencoder = ImprovedGraphSAGE(in_dim).to(device)\nedge_clf = ImprovedEdgeClassifier(256).to(device)\n\ncheckpoint = torch.load(\"/kaggle/working/gnn_optimized.pth\", map_location=device)\nencoder.load_state_dict(checkpoint['encoder'])\nedge_clf.load_state_dict(checkpoint['edge_clf'])\nencoder.eval()\nedge_clf.eval()\n\nprint(\"Model loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:21:18.860358Z","iopub.execute_input":"2025-10-15T09:21:18.860667Z","iopub.status.idle":"2025-10-15T09:21:18.882882Z","shell.execute_reply.started":"2025-10-15T09:21:18.860648Z","shell.execute_reply":"2025-10-15T09:21:18.881965Z"}},"outputs":[{"name":"stdout","text":"Model loaded\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# %% Use the loaded model to predict on test data\ndef predict_with_loaded_model():\n    \"\"\"Use ONLY the loaded gnn_optimized.pth model to predict\"\"\"\n    \n    encoder.eval()\n    edge_clf.eval()\n    \n    print(\"🧪 Using gnn_optimized.pth to predict on test data...\")\n    \n    # Take random samples from test set\n    test_samples = test_df.sample(n=50, random_state=42)\n    \n    predictions = []\n    actual_labels = []\n    \n    with torch.no_grad():\n        node_emb = encoder(data.x, data.edge_index)\n        \n        for idx, row in test_samples.iterrows():\n            u_idx = row['u']\n            v_idx = row['v']\n            true_label = row['label']\n            \n            # Get node embeddings\n            ha = node_emb[u_idx]\n            hb = node_emb[v_idx]\n            \n            # Predict using the loaded model\n            logit = edge_clf(ha.unsqueeze(0), hb.unsqueeze(0))\n            pred_prob = torch.sigmoid(logit).item()\n            pred_label = 1 if pred_prob >= 0.5 else 0\n            \n            predictions.append(pred_label)\n            actual_labels.append(true_label)\n            \n            # Print result\n            drug_a = row['drug_a']\n            drug_b = row['drug_b']\n            print(f\"{drug_a} + {drug_b} | True: {true_label} | Pred: {pred_label} | Prob: {pred_prob:.3f}\")\n    \n    # Calculate accuracy\n    accuracy = (np.array(predictions) == np.array(actual_labels)).mean()\n    print(f\"\\n📊 Accuracy: {accuracy:.4f}\")\n    \n    return predictions, actual_labels\n\n# Run prediction\npredictions, actual_labels = predict_with_loaded_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:44:59.409114Z","iopub.execute_input":"2025-10-15T09:44:59.409732Z","iopub.status.idle":"2025-10-15T09:44:59.525412Z","shell.execute_reply.started":"2025-10-15T09:44:59.409707Z","shell.execute_reply":"2025-10-15T09:44:59.524728Z"}},"outputs":[{"name":"stdout","text":"🧪 Using gnn_optimized.pth to predict on test data...\nDB01212 + DB13321 | True: 0 | Pred: 0 | Prob: 0.000\nDB00864 + DB06216 | True: 0 | Pred: 1 | Prob: 0.685\nDB00404 + DB01171 | True: 0 | Pred: 1 | Prob: 0.679\nDB01224 + DB01601 | True: 1 | Pred: 1 | Prob: 0.705\nDB00196 + DB01327 | True: 0 | Pred: 1 | Prob: 0.655\nDB00404 + DB00421 | True: 1 | Pred: 1 | Prob: 0.659\nDB00182 + DB03049 | True: 0 | Pred: 0 | Prob: 0.000\nDB00186 + DB00996 | True: 0 | Pred: 1 | Prob: 0.654\nDB08930 + DB14646 | True: 1 | Pred: 1 | Prob: 0.613\nDB00706 + DB00727 | True: 1 | Pred: 1 | Prob: 0.663\nDB00264 + DB01056 | True: 0 | Pred: 0 | Prob: 0.000\nDB00945 + DB04861 | True: 0 | Pred: 1 | Prob: 0.637\nDB00497 + DB06273 | True: 1 | Pred: 1 | Prob: 0.680\nDB00181 + DB00987 | True: 1 | Pred: 1 | Prob: 0.706\nDB00878 + DB01914 | True: 0 | Pred: 1 | Prob: 0.655\nDB00213 + DB02108 | True: 0 | Pred: 0 | Prob: 0.000\nDB04224 + DB13961 | True: 1 | Pred: 0 | Prob: 0.000\nDB00945 + DB06643 | True: 1 | Pred: 1 | Prob: 0.632\nDB00333 + DB15087 | True: 0 | Pred: 0 | Prob: 0.000\nDB00640 + DB00661 | True: 1 | Pred: 1 | Prob: 0.588\nDB00612 + DB00758 | True: 1 | Pred: 1 | Prob: 0.693\nDB00099 + DB11074 | True: 0 | Pred: 1 | Prob: 0.694\nDB06890 + DB08860 | True: 0 | Pred: 0 | Prob: 0.000\nDB01029 + DB01175 | True: 0 | Pred: 1 | Prob: 0.706\nDB00321 + DB14509 | True: 0 | Pred: 1 | Prob: 0.681\nDB00230 + DB00285 | True: 0 | Pred: 1 | Prob: 0.693\nDB00722 + DB00945 | True: 1 | Pred: 1 | Prob: 0.579\nDB00349 + DB00996 | True: 1 | Pred: 1 | Prob: 0.630\nDB00646 + DB14644 | True: 1 | Pred: 1 | Prob: 0.695\nDB00563 + DB00567 | True: 0 | Pred: 1 | Prob: 0.687\nDB00588 + DB01306 | True: 1 | Pred: 1 | Prob: 0.683\nDB01661 + DB14644 | True: 0 | Pred: 0 | Prob: 0.000\nDB00264 + DB00945 | True: 0 | Pred: 1 | Prob: 0.618\nDB00465 + DB00683 | True: 0 | Pred: 1 | Prob: 0.692\nDB00381 + DB01068 | True: 0 | Pred: 1 | Prob: 0.610\nDB01126 + DB04743 | True: 0 | Pred: 1 | Prob: 0.687\nDB15696 + DB15940 | True: 0 | Pred: 0 | Prob: 0.000\nDB00207 + DB00588 | True: 1 | Pred: 1 | Prob: 0.670\nDB00343 + DB00695 | True: 0 | Pred: 1 | Prob: 0.678\nDB00973 + DB01261 | True: 1 | Pred: 1 | Prob: 0.707\nDB00675 + DB01006 | True: 1 | Pred: 1 | Prob: 0.692\nDB00404 + DB00814 | True: 1 | Pred: 1 | Prob: 0.674\nDB00758 + DB06228 | True: 1 | Pred: 1 | Prob: 0.653\nDB00193 + DB01024 | True: 0 | Pred: 1 | Prob: 0.703\nDB01225 + DB14648 | True: 1 | Pred: 1 | Prob: 0.695\nDB00451 + DB00669 | True: 1 | Pred: 1 | Prob: 0.646\nDB00852 + DB04817 | True: 1 | Pred: 1 | Prob: 0.694\nDB00006 + DB01183 | True: 1 | Pred: 1 | Prob: 0.636\nDB00490 + DB16691 | True: 1 | Pred: 0 | Prob: 0.000\nDB00999 + DB01117 | True: 1 | Pred: 1 | Prob: 0.695\n\n📊 Accuracy: 0.6200\n","output_type":"stream"}],"execution_count":32}]}