{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13388097,"sourceType":"datasetVersion","datasetId":8495034}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:46:15.733755Z","iopub.execute_input":"2025-10-15T06:46:15.734562Z","iopub.status.idle":"2025-10-15T06:46:16.000028Z","shell.execute_reply.started":"2025-10-15T06:46:15.734525Z","shell.execute_reply":"2025-10-15T06:46:15.999238Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/polypharmacy-dataset/Side_effects_unique.csv\n/kaggle/input/polypharmacy-dataset/neg.csv\n/kaggle/input/polypharmacy-dataset/Drugbank_ID_SMILE_all_structure links.csv\n/kaggle/input/polypharmacy-dataset/pos.csv\n/kaggle/input/polypharmacy-dataset/DrugBankID2SMILES.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install rdkit torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:46:16.001303Z","iopub.execute_input":"2025-10-15T06:46:16.001607Z","iopub.status.idle":"2025-10-15T06:46:22.929548Z","shell.execute_reply.started":"2025-10-15T06:46:16.001588Z","shell.execute_reply":"2025-10-15T06:46:22.928610Z"}},"outputs":[{"name":"stdout","text":"Collecting rdkit\n  Downloading rdkit-2025.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nCollecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.3.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.15)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.9.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.1.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.8.3)\nRequirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit) (2024.2.0)\nDownloading rdkit-2025.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (36.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric, rdkit\nSuccessfully installed rdkit-2025.9.1 torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# %% Cell 1 — Imports & device\nimport os, ast, math, time, json\nfrom pathlib import Path\nfrom itertools import combinations\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# RDKit for fingerprints\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\n\n# sklearn utils\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, normalize\nfrom sklearn.model_selection import GroupShuffleSplit, train_test_split\nfrom sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, f1_score, confusion_matrix\n\n# torch + pyg\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\n# try imports for torch_geometric\ntry:\n    from torch_geometric.data import Data\n    from torch_geometric.nn import SAGEConv, global_mean_pool\nexcept Exception as e:\n    raise ImportError(\"torch_geometric not available. Install it before running. Error: \" + str(e))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:46:22.930612Z","iopub.execute_input":"2025-10-15T06:46:22.930844Z","iopub.status.idle":"2025-10-15T06:46:31.947407Z","shell.execute_reply.started":"2025-10-15T06:46:22.930822Z","shell.execute_reply":"2025-10-15T06:46:31.946612Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# %% Cell 2 — Paths to files (Kaggle dataset)\nDATA_DIR = Path('/kaggle/input/polypharmacy-dataset')\nSIDE_EFFECTS_CSV = DATA_DIR / 'Side_effects_unique.csv'\nDRUG_SMILES_CSV = DATA_DIR / 'DrugBankID2SMILES.csv'\nPOS_CSV = DATA_DIR / 'pos.csv'\nNEG_CSV = DATA_DIR / 'neg.csv'\n\nfor p in [SIDE_EFFECTS_CSV, DRUG_SMILES_CSV, POS_CSV, NEG_CSV]:\n    print(p, \"exists?\", p.exists())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:46:31.949306Z","iopub.execute_input":"2025-10-15T06:46:31.949666Z","iopub.status.idle":"2025-10-15T06:46:31.956444Z","shell.execute_reply.started":"2025-10-15T06:46:31.949648Z","shell.execute_reply":"2025-10-15T06:46:31.955590Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/polypharmacy-dataset/Side_effects_unique.csv exists? True\n/kaggle/input/polypharmacy-dataset/DrugBankID2SMILES.csv exists? True\n/kaggle/input/polypharmacy-dataset/pos.csv exists? True\n/kaggle/input/polypharmacy-dataset/neg.csv exists? True\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# %% Cell 3 — Load side-effect embeddings and normalize\nse_df = pd.read_csv(SIDE_EFFECTS_CSV, low_memory=False)\n# first two columns are UMLS and name; rest numeric\nse_ids = se_df.iloc[:, 0].astype(str).tolist()\nse_names = se_df.iloc[:, 1].astype(str).tolist()\nse_vectors = se_df.iloc[:, 2:].apply(pd.to_numeric, errors='coerce').fillna(0).values\nprint(\"SE matrix shape:\", se_vectors.shape)\n\n# Standardize (z-score)\nse_scaler = StandardScaler()\nse_vectors_scaled = se_scaler.fit_transform(se_vectors)\n# store mapping\nSE_MAP = {uid: vec for uid, vec in zip(se_ids, se_vectors_scaled)}\nSE_NAME_MAP = {uid: name for uid, name in zip(se_ids, se_names)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:46:31.957241Z","iopub.execute_input":"2025-10-15T06:46:31.957446Z","iopub.status.idle":"2025-10-15T06:46:34.260588Z","shell.execute_reply.started":"2025-10-15T06:46:31.957424Z","shell.execute_reply":"2025-10-15T06:46:34.259766Z"}},"outputs":[{"name":"stdout","text":"SE matrix shape: (7350, 768)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 🧹 Add this at the very top of your notebook to silence RDKit warnings\nfrom rdkit import RDLogger\nRDLogger.DisableLog('rdApp.*')       # hides all RDKit info/warning/deprecation logs\nimport warnings\nwarnings.filterwarnings(\"ignore\")    # hides Python-level warnings too\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:46:34.261450Z","iopub.execute_input":"2025-10-15T06:46:34.261733Z","iopub.status.idle":"2025-10-15T06:46:34.265669Z","shell.execute_reply.started":"2025-10-15T06:46:34.261714Z","shell.execute_reply":"2025-10-15T06:46:34.264995Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# %% Cell 4 replacement — fingerprints + RDKit molecular descriptors + cache\nfrom rdkit.Chem import Descriptors\nfrom rdkit import Chem, DataStructs, RDLogger\nRDLogger.DisableLog('rdApp.*')\n\nDRUG_SMILES_CSV = Path('/kaggle/input/polypharmacy-dataset/DrugBankID2SMILES.csv')\ndrug_smiles_df = pd.read_csv(DRUG_SMILES_CSV)\ndrug_smiles_df['drugbank_id'] = drug_smiles_df['drugbank_id'].astype(str)\ndrug_smiles_df['smiles'] = drug_smiles_df['smiles'].fillna('').astype(str)\n\nN_BITS = 1024\n\ndef compute_descriptors(smiles):\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return np.zeros(6, dtype=np.float32)\n        return np.array([\n            Descriptors.MolWt(mol),\n            Descriptors.TPSA(mol),\n            Descriptors.MolLogP(mol),\n            Descriptors.NumHDonors(mol),\n            Descriptors.NumHAcceptors(mol),\n            Descriptors.NumRotatableBonds(mol)\n        ], dtype=np.float32)\n    except Exception:\n        return np.zeros(6, dtype=np.float32)\n\ndef smiles_to_ecfp_bits_one(smiles, nBits=N_BITS):\n    if not isinstance(smiles, str) or smiles.strip()==\"\":\n        return np.zeros(nBits, dtype=np.uint8), True\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return np.zeros(nBits, dtype=np.uint8), True\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=nBits)\n        arr = np.zeros((nBits,), dtype=np.uint8)\n        DataStructs.ConvertToNumpyArray(fp, arr)\n        return arr, False\n    except Exception:\n        return np.zeros(nBits, dtype=np.uint8), True\n\n# compute/cached\nCACHE_FP = Path('/kaggle/working/drug_fp_desc.npz')\nif CACHE_FP.exists():\n    cache = np.load(CACHE_FP, allow_pickle=True)\n    drug_ids = cache['drug_ids'].tolist()\n    fps = cache['fps']\n    descs = cache['descs']\n    miss_flags = cache['miss_flags'].tolist()\nelse:\n    drug_ids = drug_smiles_df['drugbank_id'].tolist()\n    smiles_list = drug_smiles_df['smiles'].tolist()\n    from joblib import Parallel, delayed\n    results = Parallel(n_jobs=min(12, (os.cpu_count() or 1)), backend='loky')(\n        delayed(lambda s: (smiles_to_ecfp_bits_one(s), compute_descriptors(s)))(s) for s in smiles_list\n    )\n    fps = np.stack([r[0][0] for r in results], axis=0)\n    miss_flags = [r[0][1] for r in results]\n    descs = np.stack([r[1] for r in results], axis=0)\n    np.savez_compressed(CACHE_FP, drug_ids=drug_ids, fps=fps, descs=descs, miss_flags=miss_flags)\n\ndrug_fp_map = {db: fps[i] for i, db in enumerate(drug_ids)}\ndrug_desc_map = {db: descs[i] for i, db in enumerate(drug_ids)}\nmissing_smiles_flag = {db: bool(miss_flags[i]) for i, db in enumerate(drug_ids)}\n\nprint(\"Loaded/cached fingerprints + descriptors for\", len(drug_ids))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell 5 — Load pos/neg and expand hyperedges to pairwise edges (clique expansion)\npos_df = pd.read_csv(POS_CSV)\nneg_df = pd.read_csv(NEG_CSV)\n\ndef parse_drug_list(s):\n    # safe parse of \"['DB0001','DB0002']\"\n    try:\n        return [x.strip().strip(\"'\\\" \") for x in ast.literal_eval(s)]\n    except Exception:\n        # fallback heuristics\n        s2 = s.strip().strip('[]')\n        return [x.strip().strip(\"'\\\" \") for x in s2.split(',') if x.strip()]\n\ndef hyperedges_to_pairs(df, label):\n    rows = []\n    for _, r in df.iterrows():\n        drugs = parse_drug_list(r['DrugBankID'])\n        time_col = r.get('time', None)\n        report_id = r.get('report_id', None)\n        se_cui = r.get('SE_above_0.9', None)\n        # create all unordered pairs (i<j)\n        for a,b in combinations(sorted(set(drugs)), 2):\n            rows.append({'drug_a': a, 'drug_b': b, 'label': label, 'time': time_col, 'report_id': report_id, 'se_cui': se_cui})\n    return pd.DataFrame(rows)\n\npos_pairs = hyperedges_to_pairs(pos_df, 1)\nneg_pairs = hyperedges_to_pairs(neg_df, 0)\nedges_df = pd.concat([pos_pairs, neg_pairs], ignore_index=True).drop_duplicates(subset=['drug_a','drug_b','report_id','time'])\nprint(\"Edges shape:\", edges_df.shape)\nprint(edges_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:46:43.018717Z","iopub.execute_input":"2025-10-15T06:46:43.018999Z","iopub.status.idle":"2025-10-15T06:47:12.680817Z","shell.execute_reply.started":"2025-10-15T06:46:43.018954Z","shell.execute_reply":"2025-10-15T06:47:12.680005Z"}},"outputs":[{"name":"stdout","text":"Edges shape: (2548128, 6)\n    drug_a   drug_b  label    time report_id    se_cui\n0  DB00273  DB00472      1  2015Q4  11809573  C0151878\n1  DB00273  DB00555      1  2015Q4  11809573  C0151878\n2  DB00273  DB00557      1  2015Q4  11809573  C0151878\n3  DB00273  DB00564      1  2015Q4  11809573  C0151878\n4  DB00273  DB01050      1  2015Q4  11809573  C0151878\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# %% Cell 6 replacement — build node features and compress\nunique_drugs = sorted(set(edges_df['drug_a']).union(set(edges_df['drug_b'])))\nprint(\"Unique drugs:\", len(unique_drugs))\n\n# build fingerprint + descriptor matrices aligned to unique_drugs\nfps_mat = np.stack([drug_fp_map.get(d, np.zeros(N_BITS, dtype=np.uint8)).astype(np.float32) for d in unique_drugs])\ndesc_mat = np.stack([drug_desc_map.get(d, np.zeros(6, dtype=np.float32)) for d in unique_drugs])\n\n# robust scaling for fps then SVD compress + concat desc (normed)\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nrobust = RobustScaler()\nfps_scaled = robust.fit_transform(fps_mat)  # (N, N_BITS)\n\n# SVD on fingerprint to reduce to 256 dims\nSVD_DIM = 256\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=SVD_DIM, random_state=42)\nfps_svd = svd.fit_transform(fps_scaled)  # (N, 256)\n\n# standardize descriptors and concat\ndesc_scaler = StandardScaler()\ndesc_scaled = desc_scaler.fit_transform(desc_mat)  # (N,6)\nX_node = np.hstack([fps_svd, desc_scaled])  # final node features (N, 256+6 = 262)\nprint(\"Node features shape:\", X_node.shape)\n\n# overwrite X_fp_svd used downstream\nX_fp_svd = X_node.astype(np.float32)\ndrug_to_idx = {d: i for i, d in enumerate(unique_drugs)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:47:12.681785Z","iopub.execute_input":"2025-10-15T06:47:12.682687Z","iopub.status.idle":"2025-10-15T06:47:15.954456Z","shell.execute_reply.started":"2025-10-15T06:47:12.682661Z","shell.execute_reply":"2025-10-15T06:47:15.953670Z"}},"outputs":[{"name":"stdout","text":"Unique drugs: 12298\nNode features shape: (12298, 262)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# %% Cell 7 replacement — scale node features after SVD (important)\nfrom sklearn.preprocessing import StandardScaler\n\n# X_fp_svd already computed\nscaler_node = StandardScaler()\nX_fp_svd = scaler_node.fit_transform(X_fp_svd)   # overwrite with scaled version\nprint(\"Node feature shape after SVD & scaling:\", X_fp_svd.shape)\n\ndrug_to_idx = {d:i for i,d in enumerate(unique_drugs)}\n\n# Save scaler for later use\nimport joblib\njoblib.dump(scaler_node, \"/kaggle/working/node_scaler.joblib\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:47:15.956703Z","iopub.execute_input":"2025-10-15T06:47:15.957258Z","iopub.status.idle":"2025-10-15T06:47:16.033164Z","shell.execute_reply.started":"2025-10-15T06:47:15.957239Z","shell.execute_reply":"2025-10-15T06:47:16.032325Z"}},"outputs":[{"name":"stdout","text":"Node feature shape after SVD & scaling: (12298, 262)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/node_scaler.joblib']"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# %% Cell 8 replacement — rebuild edges and do time-aware / group split (robust)\nimport ast\nfrom itertools import combinations\nfrom sklearn.model_selection import GroupShuffleSplit, train_test_split\nimport numpy as np\n\n# --- helper to safely parse drug lists ---\ndef parse_drug_list(s):\n    try:\n        return [x.strip().strip(\"'\\\" \") for x in ast.literal_eval(s)]\n    except Exception:\n        s2 = str(s).strip().strip('[]')\n        if s2 == '' or s2.lower() == 'nan':\n            return []\n        return [x.strip().strip(\"'\\\" \") for x in s2.split(',') if x.strip()]\n\n# --- (re)create edges_df if needed ---\nif 'edges_df' not in globals():\n    # read pos/neg if not loaded yet\n    if 'pos_df' not in globals():\n        pos_df = pd.read_csv(POS_CSV, low_memory=False)\n    if 'neg_df' not in globals():\n        neg_df = pd.read_csv(NEG_CSV, low_memory=False)\n\n    def hyperedges_to_pairs(df, label):\n        rows = []\n        for _, r in df.iterrows():\n            drugs = parse_drug_list(r.get('DrugBankID', r.get('drug_list', '[]')))\n            if len(drugs) < 2:\n                continue\n            time_col = r.get('time', None)\n            report_id = r.get('report_id', None)\n            se_cui = r.get('SE_above_0.9', r.get('se_cui', None))\n            for a,b in combinations(sorted(set(drugs)), 2):\n                rows.append({'drug_a': a, 'drug_b': b, 'label': int(label),\n                             'time': time_col, 'report_id': report_id, 'se_cui': se_cui})\n        return pd.DataFrame(rows)\n\n    pos_pairs = hyperedges_to_pairs(pos_df, 1)\n    neg_pairs = hyperedges_to_pairs(neg_df, 0)\n    edges_df = pd.concat([pos_pairs, neg_pairs], ignore_index=True).drop_duplicates(subset=['drug_a','drug_b','report_id','time'])\n    print(\"Rebuilt edges_df shape:\", edges_df.shape)\nelse:\n    print(\"Using existing edges_df with shape:\", edges_df.shape)\n\n# --- create edge_labels from edges_df if missing ---\nedge_labels = edges_df[['drug_a','drug_b','label','time','report_id','se_cui']].copy()\n\n# --- ensure drug_to_idx and node indexing exist; if not create from present drugs ---\nif 'drug_to_idx' not in globals():\n    unique_drugs = sorted(set(edge_labels['drug_a']).union(set(edge_labels['drug_b'])))\n    drug_to_idx = {d:i for i,d in enumerate(unique_drugs)}\n    print(\"Created drug_to_idx for\", len(unique_drugs), \"drugs\")\n\n# map to integer node ids 'u' and 'v'\nedge_labels['u'] = edge_labels['drug_a'].map(drug_to_idx).astype('Int64')\nedge_labels['v'] = edge_labels['drug_b'].map(drug_to_idx).astype('Int64')\n\n# drop any rows with missing mapping (if some drug not found)\nbefore = len(edge_labels)\nedge_labels = edge_labels.dropna(subset=['u','v']).reset_index(drop=True)\nafter = len(edge_labels)\nif after < before:\n    print(f\"Dropped {before-after} edges due to missing drug->idx mapping\")\n\n# --- Now do time-aware split if time exists, else group split by report_id ---\ndef time_to_ordinal(t):\n    try:\n        ts = str(t)\n        if 'Q' in ts:\n            y,q = ts.split('Q')\n            return int(y)*10 + int(q)\n        return int(float(ts))\n    except Exception:\n        return 0\n\nif 'time' in edge_labels.columns and edge_labels['time'].notnull().any():\n    edge_labels['time_ord'] = edge_labels['time'].apply(time_to_ordinal)\n    times_sorted = sorted(edge_labels['time_ord'].unique())\n    cutoff_idx = max(1, int(len(times_sorted)*0.7))\n    cutoff_time = times_sorted[min(cutoff_idx, len(times_sorted)-1)]\n    train_mask = edge_labels['time_ord'] <= cutoff_time\n    rest = ~train_mask\n    rest_idx = edge_labels[rest].index\n    # stratify only if both classes present in 'rest'\n    stratify_param = edge_labels.loc[rest, 'label'] if edge_labels.loc[rest, 'label'].nunique() > 1 else None\n    val_idx, test_idx = train_test_split(rest_idx, test_size=0.5, random_state=42, stratify=stratify_param)\n    val_mask = edge_labels.index.isin(val_idx)\n    test_mask = edge_labels.index.isin(test_idx)\nelse:\n    gss = GroupShuffleSplit(n_splits=1, train_size=0.7, random_state=42)\n    groups = edge_labels['report_id'].fillna('_no_report_')\n    train_idx, other_idx = next(gss.split(edge_labels, edge_labels['label'], groups=groups))\n    # stratify only if both classes present in other_idx\n    strat = edge_labels.loc[other_idx, 'label'] if edge_labels.loc[other_idx, 'label'].nunique() > 1 else None\n    val_idx, test_idx = train_test_split(other_idx, test_size=0.5, random_state=42, stratify=strat)\n    train_mask = edge_labels.index.isin(train_idx)\n    val_mask = edge_labels.index.isin(val_idx)\n    test_mask = edge_labels.index.isin(test_idx)\n\nedge_labels['split'] = np.where(train_mask, 'train', np.where(val_mask, 'val', 'test'))\nprint(edge_labels['split'].value_counts())\n# expose train/val/test dfs\ntrain_df = edge_labels[edge_labels['split']=='train'].reset_index(drop=True)\nval_df = edge_labels[edge_labels['split']=='val'].reset_index(drop=True)\ntest_df = edge_labels[edge_labels['split']=='test'].reset_index(drop=True)\nprint(\"train/val/test sizes:\", len(train_df), len(val_df), len(test_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:47:16.034036Z","iopub.execute_input":"2025-10-15T06:47:16.034396Z","iopub.status.idle":"2025-10-15T06:47:22.122623Z","shell.execute_reply.started":"2025-10-15T06:47:16.034371Z","shell.execute_reply":"2025-10-15T06:47:22.121929Z"}},"outputs":[{"name":"stdout","text":"Using existing edges_df with shape: (2548128, 6)\nsplit\ntrain    1954270\nval       296929\ntest      296929\nName: count, dtype: int64\ntrain/val/test sizes: 1954270 296929 296929\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# %% Cell 9 replacement — build edge_index_t from edge_labels and create PyG Data\nimport torch\nfrom torch_geometric.utils import remove_self_loops\n\n# ensure edge_labels with 'u' and 'v' exists\nassert 'edge_labels' in globals(), \"edge_labels missing — run the edge-building cell first.\"\nassert 'X_fp_svd' in globals(), \"X_fp_svd missing — run SVD / node feature cell first.\"\n\n# build undirected adjacency from edge_labels (use all observed edges for message passing)\nu_arr = edge_labels['u'].astype(int).to_numpy()\nv_arr = edge_labels['v'].astype(int).to_numpy()\n\n# stack both directions\nedge_index_np = np.vstack([np.concatenate([u_arr, v_arr]), np.concatenate([v_arr, u_arr])]).astype(np.int64)\n# remove self-loops if any\nedge_index_tensor = torch.tensor(edge_index_np, dtype=torch.long)\nedge_index_tensor, _ = remove_self_loops(edge_index_tensor)\n\n# create Data object\ndata = Data(x=torch.tensor(X_fp_svd, dtype=torch.float), edge_index=edge_index_tensor).to(device)\nedge_index_t = edge_index_tensor  # keep variable name used elsewhere\nprint(\"Data created:\", data)\nprint(\"Num nodes:\", data.num_nodes, \"Num edges (directed):\", data.num_edges)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:47:22.123357Z","iopub.execute_input":"2025-10-15T06:47:22.123546Z","iopub.status.idle":"2025-10-15T06:47:22.556038Z","shell.execute_reply.started":"2025-10-15T06:47:22.123532Z","shell.execute_reply":"2025-10-15T06:47:22.555389Z"}},"outputs":[{"name":"stdout","text":"Data created: Data(x=[12298, 262], edge_index=[2, 5096256])\nNum nodes: 12298 Num edges (directed): 5096256\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# %% Cell 10 replacement — stronger encoder + classifier (3-layer, larger)\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\nclass GraphSAGEEncoder(nn.Module):\n    def __init__(self, in_channels, hidden_channels=512, n_layers=3, dropout=0.2):\n        super().__init__()\n        self.convs = nn.ModuleList()\n        self.bns = nn.ModuleList()\n        self.convs.append(SAGEConv(in_channels, hidden_channels))\n        self.bns.append(nn.BatchNorm1d(hidden_channels))\n        for _ in range(n_layers-1):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n            self.bns.append(nn.BatchNorm1d(hidden_channels))\n        self.act = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x, edge_index):\n        for conv, bn in zip(self.convs, self.bns):\n            x = conv(x, edge_index)\n            x = bn(x)\n            x = self.act(x)\n            x = self.dropout(x)\n        return x\n\nclass EdgeClassifier(nn.Module):\n    def __init__(self, node_emb_dim, hidden=256, dropout=0.2):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(node_emb_dim*2, hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, hidden//2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden//2, 1)\n        )\n    def forward(self, ha, hb):\n        h = torch.cat([ha, hb], dim=1)\n        return self.mlp(h).squeeze(1)\n\nin_dim = X_fp_svd.shape[1]\nencoder = encoder.to(device)\nedge_clf = edge_clf.to(device)\ndata = data.to(device)\n\nencoder = GraphSAGEEncoder(in_dim, hidden_channels=512, n_layers=3, dropout=0.2).to(device)\nedge_clf = EdgeClassifier(node_emb_dim=512, hidden=256, dropout=0.2).to(device)\n\n# optimizer + scheduler\noptimizer = torch.optim.AdamW(list(encoder.parameters()) + list(edge_clf.parameters()), lr=5e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:51:48.015817Z","iopub.execute_input":"2025-10-15T06:51:48.016524Z","iopub.status.idle":"2025-10-15T06:51:48.054214Z","shell.execute_reply.started":"2025-10-15T06:51:48.016499Z","shell.execute_reply":"2025-10-15T06:51:48.053482Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# %% Cell 11 replacement — balanced batches with oversampling (more positive presence)\ntrain_df = edge_labels[edge_labels['split']=='train'].reset_index(drop=True)\nval_df = edge_labels[edge_labels['split']=='val'].reset_index(drop=True)\ntest_df = edge_labels[edge_labels['split']=='test'].reset_index(drop=True)\n\ntrain_df['u'] = train_df['u'].astype(int); train_df['v'] = train_df['v'].astype(int)\nval_df['u'] = val_df['u'].astype(int); val_df['v'] = val_df['v'].astype(int)\ntest_df['u'] = test_df['u'].astype(int); test_df['v'] = test_df['v'].astype(int)\n\ntrain_pos_idx = train_df[train_df['label']==1].index.to_numpy()\ntrain_neg_idx = train_df[train_df['label']==0].index.to_numpy()\n\n# oversample positives to reach a target positive ratio per batch (e.g., 40%)\ndef balanced_edge_batches(df, pos_idx, neg_idx, batch_size=4096, pos_ratio=0.4):\n    pos_per = int(batch_size * pos_ratio)\n    neg_per = batch_size - pos_per\n    # create repeated pools\n    while True:\n        p = np.random.choice(pos_idx, size=max(len(pos_idx), pos_per*2), replace=True)\n        n = np.random.choice(neg_idx, size=max(len(neg_idx), neg_per*2), replace=True)\n        # iterate in chunks\n        for i in range(0, max(len(p), len(n)), max(pos_per, neg_per)):\n            pos_chunk = p[i:i+pos_per]\n            neg_chunk = n[i:i+neg_per]\n            chosen = np.concatenate([pos_chunk, neg_chunk])\n            np.random.shuffle(chosen)\n            sub = df.loc[chosen]\n            u = torch.tensor(sub['u'].values, dtype=torch.long, device=device)\n            v = torch.tensor(sub['v'].values, dtype=torch.long, device=device)\n            y = torch.tensor(sub['label'].astype(np.float32).values, dtype=torch.float, device=device)\n            yield u, v, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:47:22.631153Z","iopub.execute_input":"2025-10-15T06:47:22.631328Z","iopub.status.idle":"2025-10-15T06:47:24.694020Z","shell.execute_reply.started":"2025-10-15T06:47:22.631314Z","shell.execute_reply":"2025-10-15T06:47:24.693396Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# %% Cell 12 — mini-batch GNN training without torch-sparse/pyg-lib\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score\nfrom tqdm import tqdm\n\nEPOCHS = 50\nBATCH_SIZE = 2048   # reduce if OOM\nbest_val_auc = -float('inf')\npatience, pat = 6, 0\n\nfor ep in range(1, EPOCHS + 1):\n    encoder.train(); edge_clf.train()\n    torch.cuda.empty_cache()\n    epoch_losses = []\n    pbar = tqdm(range(0, len(train_df), BATCH_SIZE), desc=f\"Epoch {ep}\")\n\n    for start in pbar:\n        end = min(start + BATCH_SIZE, len(train_df))\n        batch = train_df.iloc[start:end]\n\n        # subgraph of drugs in this mini-batch\n        nodes = np.unique(np.concatenate([batch['u'].values, batch['v'].values]))\n        idx_map = {n: i for i, n in enumerate(nodes)}\n        sub_x = data.x[nodes]\n        mask = np.isin(edge_index_t[0].cpu(), nodes) & np.isin(edge_index_t[1].cpu(), nodes)\n        sub_edge = edge_index_t[:, mask]\n        sub_edge = torch.stack([torch.tensor([idx_map[i.item()] for i in sub_edge[0]]),\n                                torch.tensor([idx_map[i.item()] for i in sub_edge[1]])]).to(device)\n\n        sub_x = sub_x.to(device)\n        node_emb = encoder(sub_x, sub_edge)\n\n        u = torch.tensor([idx_map[i] for i in batch['u'].values], device=device)\n        v = torch.tensor([idx_map[i] for i in batch['v'].values], device=device)\n        y = torch.tensor(batch['label'].values, dtype=torch.float, device=device)\n\n        optimizer.zero_grad()\n        logits = edge_clf(node_emb[u], node_emb[v])\n        loss = F.binary_cross_entropy_with_logits(logits, y)\n        loss.backward()\n        optimizer.step()\n        epoch_losses.append(loss.item())\n\n    # ----- validation -----\n    encoder.eval(); edge_clf.eval()\n    with torch.no_grad():\n        val_probs, val_true = [], []\n        for start in range(0, len(val_df), BATCH_SIZE):\n            end = min(start + BATCH_SIZE, len(val_df))\n            b = val_df.iloc[start:end]\n            u = torch.tensor(b['u'].values, device=device)\n            v = torch.tensor(b['v'].values, device=device)\n            logits = edge_clf(encoder(data.x.to(device), data.edge_index)[u],\n                              encoder(data.x.to(device), data.edge_index)[v])\n            probs = torch.sigmoid(logits).cpu().numpy()\n            val_probs.extend(probs); val_true.extend(b['label'].values)\n\n        val_auc = roc_auc_score(val_true, val_probs)\n        val_ap = average_precision_score(val_true, val_probs)\n        val_f1 = f1_score(val_true, (np.array(val_probs) >= 0.5).astype(int))\n        print(f\"Epoch {ep:02d} | loss {np.mean(epoch_losses):.4f} | val_auc {val_auc:.4f} | val_ap {val_ap:.4f} | val_f1 {val_f1:.4f}\")\n\n    # early stopping\n    if val_auc > best_val_auc + 1e-4:\n        best_val_auc, pat = val_auc, 0\n        torch.save({'encoder': encoder.state_dict(), 'edge_clf': edge_clf.state_dict()},\n                   \"/kaggle/working/gnn_best_simple.pth\")\n    else:\n        pat += 1\n        if pat >= patience:\n            print(\"Early stopping\")\n            break\n\nckp = torch.load(\"/kaggle/working/gnn_best_simple.pth\", map_location=device)\nencoder.load_state_dict(ckp['encoder']); edge_clf.load_state_dict(ckp['edge_clf'])\nprint(\"Loaded best model, best_val_auc=\", best_val_auc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:13:32.814121Z","iopub.execute_input":"2025-10-15T07:13:32.815058Z","iopub.status.idle":"2025-10-15T07:18:58.812261Z","shell.execute_reply.started":"2025-10-15T07:13:32.815025Z","shell.execute_reply":"2025-10-15T07:18:58.811524Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 4/4 [00:10<00:00,  2.67s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | loss 0.6711 | val_auc 0.4968 | val_ap 0.5031 | val_f1 0.4970\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 4/4 [00:10<00:00,  2.69s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | loss 0.6667 | val_auc 0.4983 | val_ap 0.5039 | val_f1 0.4992\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 4/4 [00:10<00:00,  2.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | loss 0.6617 | val_auc 0.4992 | val_ap 0.5047 | val_f1 0.5103\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 4/4 [00:10<00:00,  2.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | loss 0.6560 | val_auc 0.4993 | val_ap 0.5049 | val_f1 0.5152\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 4/4 [00:11<00:00,  2.77s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | loss 0.6494 | val_auc 0.4991 | val_ap 0.5046 | val_f1 0.5153\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | loss 0.6418 | val_auc 0.4988 | val_ap 0.5042 | val_f1 0.5152\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 4/4 [00:11<00:00,  2.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | loss 0.6337 | val_auc 0.4990 | val_ap 0.5044 | val_f1 0.5191\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 4/4 [00:10<00:00,  2.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | loss 0.6257 | val_auc 0.4999 | val_ap 0.5040 | val_f1 0.5068\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 4/4 [00:10<00:00,  2.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | loss 0.6195 | val_auc 0.5019 | val_ap 0.5026 | val_f1 0.4097\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 4/4 [00:10<00:00,  2.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | loss 0.6232 | val_auc 0.5009 | val_ap 0.5006 | val_f1 0.4548\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 4/4 [00:10<00:00,  2.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | loss 0.6177 | val_auc 0.4991 | val_ap 0.4994 | val_f1 0.5664\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 4/4 [00:10<00:00,  2.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | loss 0.6042 | val_auc 0.5024 | val_ap 0.5010 | val_f1 0.4619\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 4/4 [00:10<00:00,  2.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | loss 0.5978 | val_auc 0.5013 | val_ap 0.5010 | val_f1 0.5496\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 4/4 [00:10<00:00,  2.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | loss 0.5889 | val_auc 0.5030 | val_ap 0.5014 | val_f1 0.4825\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 4/4 [00:10<00:00,  2.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | loss 0.5816 | val_auc 0.5017 | val_ap 0.5005 | val_f1 0.5412\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 4/4 [00:11<00:00,  2.76s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | loss 0.5745 | val_auc 0.5031 | val_ap 0.5000 | val_f1 0.4719\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | loss 0.5670 | val_auc 0.5024 | val_ap 0.4987 | val_f1 0.5277\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 4/4 [00:11<00:00,  2.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | loss 0.5593 | val_auc 0.5035 | val_ap 0.4980 | val_f1 0.5020\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | loss 0.5519 | val_auc 0.5042 | val_ap 0.4967 | val_f1 0.4926\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | loss 0.5451 | val_auc 0.5040 | val_ap 0.4954 | val_f1 0.5109\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 4/4 [00:10<00:00,  2.69s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | loss 0.5373 | val_auc 0.5038 | val_ap 0.4952 | val_f1 0.5282\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 4/4 [00:10<00:00,  2.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | loss 0.5305 | val_auc 0.5037 | val_ap 0.4949 | val_f1 0.5287\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | loss 0.5238 | val_auc 0.5040 | val_ap 0.4941 | val_f1 0.4970\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 4/4 [00:10<00:00,  2.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | loss 0.5188 | val_auc 0.5054 | val_ap 0.4935 | val_f1 0.4520\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | loss 0.5208 | val_auc 0.5053 | val_ap 0.4926 | val_f1 0.4630\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 | loss 0.5237 | val_auc 0.5024 | val_ap 0.4909 | val_f1 0.5695\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 4/4 [00:11<00:00,  2.77s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 | loss 0.5266 | val_auc 0.5042 | val_ap 0.4930 | val_f1 0.5099\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 4/4 [00:10<00:00,  2.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 | loss 0.4988 | val_auc 0.5046 | val_ap 0.4931 | val_f1 0.4967\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 4/4 [00:11<00:00,  2.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 | loss 0.4906 | val_auc 0.5035 | val_ap 0.4919 | val_f1 0.5339\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 4/4 [00:10<00:00,  2.69s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 30 | loss 0.4850 | val_auc 0.5049 | val_ap 0.4923 | val_f1 0.4668\nEarly stopping\nLoaded best model, best_val_auc= 0.5054440670371392\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"!pip install torch-sparse -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n!pip install torch-geometric\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# add right before for-epoch loop\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\ntorch.set_num_threads(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:13:28.498341Z","iopub.execute_input":"2025-10-15T07:13:28.498893Z","iopub.status.idle":"2025-10-15T07:13:28.502491Z","shell.execute_reply.started":"2025-10-15T07:13:28.498868Z","shell.execute_reply":"2025-10-15T07:13:28.501646Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Improved training cell for higher accuracy\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score\nfrom tqdm import tqdm\n\nEPOCHS = 80\nLR = 1e-3\noptimizer = torch.optim.AdamW(list(encoder.parameters()) + list(edge_clf.parameters()), lr=LR, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\nbest_auc, patience, counter = 0, 8, 0\n\nfor ep in range(1, EPOCHS + 1):\n    encoder.train(); edge_clf.train()\n    total_loss = 0\n    for u_batch, v_batch, y_batch in balanced_edge_batches(train_df, train_pos_idx, train_neg_idx, batch_size=4096, pos_ratio=0.5):\n        optimizer.zero_grad()\n        ha, hb = encoder(data.x.to(device), data.edge_index)[u_batch], encoder(data.x.to(device), data.edge_index)[v_batch]\n        logits = edge_clf(ha, hb)\n        loss = F.binary_cross_entropy_with_logits(logits, y_batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    scheduler.step()\n    \n    # --- Validation ---\n    encoder.eval(); edge_clf.eval()\n    with torch.no_grad():\n        emb = encoder(data.x.to(device), data.edge_index)\n        vu, vv = torch.tensor(val_df['u'].values, device=device), torch.tensor(val_df['v'].values, device=device)\n        vy = torch.tensor(val_df['label'].values, dtype=torch.float, device=device)\n        val_probs = torch.sigmoid(edge_clf(emb[vu], emb[vv])).cpu().numpy()\n        val_auc = roc_auc_score(vy.cpu().numpy(), val_probs)\n        val_ap = average_precision_score(vy.cpu().numpy(), val_probs)\n        val_f1 = f1_score(vy.cpu().numpy(), (val_probs >= 0.5).astype(int))\n    \n    print(f\"Epoch {ep:03d} | loss {total_loss:.4f} | val_auc {val_auc:.4f} | val_ap {val_ap:.4f} | val_f1 {val_f1:.4f}\")\n\n    if val_auc > best_auc:\n        best_auc = val_auc\n        torch.save({'encoder': encoder.state_dict(), 'edge_clf': edge_clf.state_dict()}, \"/kaggle/working/gnn_best_strong.pth\")\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping — no improvement.\")\n            break\n\nprint(f\"✅ Training complete. Best validation AUC = {best_auc:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-15T07:45:35.898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% Cell — Final Test / Evaluation / Prediction\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score, classification_report\nimport numpy as np, torch\n\n# 🔹 Load best trained model\nckp = torch.load(\"/kaggle/working/gnn_best_simple.pth\", map_location=device)\nencoder.load_state_dict(ckp[\"encoder\"])\nedge_clf.load_state_dict(ckp[\"edge_clf\"])\nencoder.eval(); edge_clf.eval()\n\n# 🔹 Compute node embeddings once\nwith torch.no_grad():\n    node_emb = encoder(data.x.to(device), data.edge_index).cpu()\n\n# 🔹 Predict probabilities for test set\ndef predict_gnn(df):\n    probs = []\n    for _, r in df.iterrows():\n        ua, vb = int(r[\"u\"]), int(r[\"v\"])\n        ha, hb = node_emb[ua], node_emb[vb]\n        p = torch.sigmoid(edge_clf(ha.to(device), hb.to(device))).cpu().item()\n        probs.append(p)\n    return np.array(probs)\n\nprint(\"Running predictions on test data...\")\ntest_probs = predict_gnn(test_df)\ntest_labels = test_df[\"label\"].values\ntest_preds = (test_probs >= 0.5).astype(int)\n\n# 🔹 Evaluate\ntest_auc = roc_auc_score(test_labels, test_probs)\ntest_ap = average_precision_score(test_labels, test_probs)\ntest_f1 = f1_score(test_labels, test_preds)\n\nprint(f\"✅ Test AUC: {test_auc:.4f} | AP: {test_ap:.4f} | F1: {test_f1:.4f}\")\nprint(classification_report(test_labels, test_preds))\n\n# 🔹 Example prediction for custom input\nsample_input = {\"report_id\": \"runtime-12345\", \"drugs\": [\"DB00006\", \"DB00341\", \"DB01118\"]}\npairs = [(sample_input[\"drugs\"][i], sample_input[\"drugs\"][j])\n         for i in range(len(sample_input[\"drugs\"]))\n         for j in range(i+1, len(sample_input[\"drugs\"]))]\n\nprint(\"\\nPredicted ADR severity (0=Low,1=High):\")\nfor a,b in pairs:\n    if a in drug_to_idx and b in drug_to_idx:\n        pa = drug_to_idx[a]; pb = drug_to_idx[b]\n        p = torch.sigmoid(edge_clf(node_emb[pa].to(device), node_emb[pb].to(device))).cpu().item()\n        print(f\"{a} + {b} → severity score: {p:.3f}\")\n    else:\n        print(f\"{a} + {b} → unknown drug ID\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-15T07:45:35.898Z"}},"outputs":[],"execution_count":null}]}